{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit Learn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "#Natural Language Toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "# Allow plots in Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Two private loans have with them very discharg...\n",
       "1    attach a letter dated explaining dropped the v...\n",
       "2    Please see attached Complaint Number against c...\n",
       "3    feel as though 've been subjected to predatory...\n",
       "4    a veteran living on social security and cosign...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corps = pd.read_csv(\"D:/Documents/Data/case_study_data_tiny.csv\")\n",
    "#df = corps[['complaint_id','product_group','text']]\n",
    "df = corps[['product_group','text']]\n",
    "X = df['text']\n",
    "y = df['product_group']\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    two private loans have with them very discharg...\n",
       "1    attach a letter dated explaining dropped the v...\n",
       "2    please see attached complaint number against c...\n",
       "3    feel as though 've been subjected to predatory...\n",
       "4    a veteran living on social security and cosign...\n",
       "5    problem this company has been transfered a deb...\n",
       "6    have filed a complaint before case was hit wit...\n",
       "7    this is about the three major credit agencies ...\n",
       "8    the office of the attorney general office is r...\n",
       "9    to whom it may concern am in need of some help...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= X.str.lower()\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "X = X.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [two, private, loans, have, with, them, very, ...\n",
       "1    [attach, a, letter, dated, explaining, dropped...\n",
       "2    [please, see, attached, complaint, number, aga...\n",
       "3    [feel, as, though, 've, been, subjected, to, p...\n",
       "4    [a, veteran, living, on, social, security, and...\n",
       "5    [problem, this, company, has, been, transfered...\n",
       "6    [have, filed, a, complaint, before, case, was,...\n",
       "7    [this, is, about, the, three, major, credit, a...\n",
       "8    [the, office, of, the, attorney, general, offi...\n",
       "9    [to, whom, it, may, concern, am, in, need, of,...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isn', 'as', 'do', 'd', 're', 'yourself', 'doesn', 'on', 'am', \"haven't\", \"you've\", 'all', 'an', 'this', 'where', 'wouldn', 'which', \"aren't\", 'from', 'below', 'why', \"that'll\", 'nor', 'myself', 'have', 'haven', 'what', 'shan', 'wasn', 'couldn', 'or', 'the', 'again', \"mightn't\", 's', 'same', 'o', 'being', 'm', 'off', 'herself', 'down', 'into', 'both', 'themselves', 'should', 'here', \"hasn't\", 'yours', 'them', 'll', 'does', 'under', \"didn't\", 'was', 'how', 'not', 'up', 'ours', 'they', 'more', 'doing', 'but', 'at', 'y', \"hadn't\", 'himself', 'aren', 'him', 'won', \"you're\", \"isn't\", 'that', 'very', \"she's\", 'their', 'there', 'you', 'her', 'now', 'ain', 'ma', 'during', 'once', 'than', \"couldn't\", 'these', 'he', 'before', 'ourselves', 'are', 'such', 'will', 'too', \"it's\", 'my', \"you'll\", 'weren', 'only', \"you'd\", 'having', 'because', \"shan't\", 'who', 'is', 'while', \"doesn't\", 'be', 'mustn', 'over', 'own', 'when', 'through', 'don', 'and', 'his', 'our', 'it', 'shouldn', \"needn't\", 'has', \"mustn't\", 've', 'she', 'to', 'few', \"weren't\", 'hasn', 'theirs', \"won't\", 'whom', 'by', 'against', 'no', 'itself', 'further', 'just', 'we', 'in', 'above', 'so', \"should've\", 'of', 'between', 'about', 'out', 'each', 'hers', \"don't\", 't', 'then', 'can', 'been', 'after', 'some', 'hadn', 'i', 'its', 'for', 'any', 'a', 'with', 'didn', \"wouldn't\", 'me', 'had', 'most', \"wasn't\", \"shouldn't\", 'your', 'until', 'yourselves', 'needn', 'mightn', 'were', 'other', 'if', 'did', 'those'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en = set(stopwords.words('english')) # Set checking is faster in Python than list.\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "# It's a string so we have to them into a set type\n",
    "print('From string.punctuation:', type(punctuation), punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isn', 'as', ';', 'do', 'd', 're', 'yourself', 'doesn', \"'\", '(', '-', 'on', 'am', \"haven't\", \"you've\", 'all', 'an', 'this', 'where', '*', '<', '\\\\', ']', 'wouldn', 'which', \"aren't\", 'from', 'below', 'why', '>', \"that'll\", '@', '[', 'nor', 'myself', 'have', 'haven', '=', 'what', 'shan', 'wasn', '\"', 'couldn', 'or', 'the', 'again', \"mightn't\", 's', 'same', 'o', 'being', 'm', '_', 'off', 'herself', 'down', 'into', 'both', 'themselves', 'should', 'here', \"hasn't\", '+', 'yours', '~', 'them', 'll', 'does', 'under', \"didn't\", 'was', 'how', 'not', 'up', 'ours', 'they', 'more', 'doing', 'but', 'at', 'y', \"hadn't\", 'himself', 'aren', 'him', 'won', \"you're\", \"isn't\", ':', '!', 'that', '}', 'very', '$', \"she's\", 'their', 'there', '#', 'you', ')', 'her', 'now', 'ain', 'ma', 'during', '&', 'once', 'than', \"couldn't\", 'these', 'he', 'before', 'ourselves', 'are', 'such', 'will', 'too', \"it's\", 'my', \"you'll\", 'weren', 'only', \"you'd\", 'having', 'because', \"shan't\", 'who', 'is', 'while', \"doesn't\", 'be', 'mustn', 'over', 'own', 'when', 'through', 'don', 'and', 'his', 'our', 'it', 'shouldn', ',', \"needn't\", 'has', \"mustn't\", 've', 'she', 'to', 'few', '|', '%', \"weren't\", 'hasn', 'theirs', \"won't\", '`', 'whom', 'by', 'against', 'no', 'itself', 'further', 'just', 'we', '?', 'in', '.', 'above', 'so', \"should've\", 'of', 'between', 'about', 'out', 'each', 'hers', \"don't\", 't', 'then', 'can', 'been', 'after', 'some', 'hadn', 'i', 'its', 'for', 'any', 'a', 'with', 'didn', '{', \"wouldn't\", 'me', 'had', '/', 'most', \"wasn't\", \"shouldn't\", 'your', '^', 'until', 'yourselves', 'needn', 'mightn', 'were', 'other', 'if', 'did', 'those'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n",
    "print(stopwords_en_withpunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [two, private, loans, discharged, chapter, ban...\n",
       "1    [attach, letter, dated, explaining, dropped, v...\n",
       "2    [please, see, attached, complaint, number, cop...\n",
       "3    [feel, though, 've, subjected, predatory, loan...\n",
       "4    [veteran, living, social, security, cosigned, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords_en_withpunct\n",
    "X = X.apply(lambda x: [item for item in x if item not in stop])\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Use English stemmer.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "X = X.apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "#df = df.drop(columns=['unstemmed']) # Get rid of the unstemmed column.\n",
    "#df # Print dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [two, privat, loan, discharg, chapter, bankrup...\n",
       "1    [attach, letter, date, explain, drop, vehicl, ...\n",
       "2    [pleas, see, attach, complaint, number, copi, ...\n",
       "3    [feel, though, ve, subject, predatori, loan, r...\n",
       "4    [veteran, live, social, secur, cosign, loan, d...\n",
       "5    [problem, compani, transfer, debt, unabl, prov...\n",
       "6    [file, complaint, case, hit, doubl, whammi, ye...\n",
       "7    [three, major, credit, agenc, error, get, fix,...\n",
       "8    [offic, attorney, general, offic, report, owe,...\n",
       "9    [may, concern, need, help, privat, loan, ae, 1...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 49999)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = pd.DataFrame(y)\n",
    "# X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-53af6680a01a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbigram_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_bigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-53af6680a01a>\u001b[0m in \u001b[0;36mfind_bigrams\u001b[1;34m(input_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbigram_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mbigram_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbigram_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "input_list = X.tolist()\n",
    "\n",
    "def find_bigrams(input_list):\n",
    "    bigram_list = []\n",
    "    for i in range(len(input_list)-1):\n",
    "        bigram_list.append((input_list[i], input_list[i+1]))\n",
    "    return bigram_list\n",
    "\n",
    "mapped = find_bigrams(input_list)\n",
    "print(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_list)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = corps\n",
    "df = df[pd.notnull(df['product_group'])]\n",
    "df.columns = ['product_group','text']\n",
    "df['category_id'] = df['product_group'].factorize()[0]\n",
    "category_id_df = df[['product_group', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'product_group']].values)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CountVectorizer(\n",
    "    input='corps',            #the input text data or corpus\n",
    "    encoding='utf-8',         #encoding is used to decode\n",
    "    decode_error='strict',    #means a UnicodeDecodeError will be raised (other values are ignore and replace) \n",
    "    strip_accents='ascii',    #removes accents and perform other character normalization (ascii is the fastest)\n",
    "    lowercase=True,           #converts all tect to lower case\n",
    "    tokenizer=word_tokenize,  #default value is None (only applies if analyzer == 'word')\n",
    "    stop_words='english',     #default value is None (only applies if analyzer == 'word')\n",
    "    token_pattern=r'\\b\\w+\\b', #string, denoting what constitutes a “token” (only used if analyzer == 'word')\n",
    "    ngram_range=(1, 3),       #will yield unigrams, bigrams, and trigrams\n",
    "    analyzer='word',          #feature makeup {‘string’, ‘word’, ‘char’, ‘char_wb’} or callable\n",
    "    max_df=1.0,               #ignore terms that have a frequency higher than this threshold\n",
    "    min_df=1,                 #ignore terms that have a frequency lower than this threshold\n",
    "    max_features = 20,        #build a vocabulary size N or None\n",
    "    binary=False,             #if True, all non zero counts are set to 1\n",
    "    dtype= np.int64           #type of the matrix returned by fit_transform() or transform()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vec = CountVectorizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cnt_vec.fit_transform(pd.DataFrame(X_train)) #CORRECT TRANSFORM\n",
    "#labels = pd.DataFrame(y_train) #CORRECT LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "TfidfTransformer(\n",
    "    norm ='l2',          #each output row will have unit norm, either: 'l2', 'l1', or None\n",
    "    use_idf=True,        #enable inverse-document-frequency reweighting\n",
    "    smooth_idf=True,     #smooth idf weights by adding one to document frequencies to prevent divison by zero\n",
    "    sublinear_tf=False,  #apply sublinear tf scaling, i.e. replace tf with 1 + log(tf)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE#\n",
    "tfidf= TfidfVectorizer(sublinear_tf=True,  norm='l2', encoding='latin-1', ngram_range=(1, 3), stop_words='english')\n",
    "#features = tfidf.fit_transform(X_train) #CORRECT TRANSFORM\n",
    "#labels = y_train #CORRECT LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf.fit_transform(pd.DataFrame(X_train)) #CORRECT TRANSFORM\n",
    "labels = pd.DataFrame(y_train) #CORRECT LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf.inverse_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "for product_group, category_id in sorted(id_to_category.items()):\n",
    "  features_chi2 = chi2(features,labels == category_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
    "  print(\"# '{}':\".format(category_id))\n",
    "  print(\".   Most correlated unigrams:\\n.     {}\".format('\\n.     '.join(unigrams[-N:])))\n",
    "  print(\".   Most correlated bigrams:\\n.     {}\".format('\\n.     '.join(bigrams[-N:])))\n",
    "  print(\".   Most correlated trigrams:\\n.     {}\".format('\\n.     '.join(trigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
