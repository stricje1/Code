{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'bank_service':\n",
      ".   Most correlated unigrams:\n",
      ".     deposit\n",
      ".     overdraft\n",
      ".   Most correlated bigrams:\n",
      ".     overdraft fees\n",
      ".     checking account\n",
      ".   Most correlated trigrams:\n",
      ".     charged overdraft fees\n",
      ".     opened checking account\n",
      "# 'credit_card':\n",
      ".   Most correlated unigrams:\n",
      ".     express\n",
      ".     card\n",
      ".   Most correlated bigrams:\n",
      ".     american express\n",
      ".     credit card\n",
      ".   Most correlated trigrams:\n",
      ".     credit card account\n",
      ".     credit card company\n",
      "# 'credit_reporting':\n",
      ".   Most correlated unigrams:\n",
      ".     experian\n",
      ".     equifax\n",
      ".   Most correlated bigrams:\n",
      ".     credit file\n",
      ".     credit report\n",
      ".   Most correlated trigrams:\n",
      ".     mistakes appear report\n",
      ".     appear report understanding\n",
      "# 'debt_collection':\n",
      ".   Most correlated unigrams:\n",
      ".     collection\n",
      ".     debt\n",
      ".   Most correlated bigrams:\n",
      ".     collect debt\n",
      ".     collection agency\n",
      ".   Most correlated trigrams:\n",
      ".     attempting collect debt\n",
      ".     trying collect debt\n",
      "# 'loan':\n",
      ".   Most correlated unigrams:\n",
      ".     loans\n",
      ".     navient\n",
      ".   Most correlated bigrams:\n",
      ".     student loans\n",
      ".     student loan\n",
      ".   Most correlated trigrams:\n",
      ".     based repayment plan\n",
      ".     income based repayment\n",
      "# 'money_transfers':\n",
      ".   Most correlated unigrams:\n",
      ".     western\n",
      ".     coinbase\n",
      ".   Most correlated bigrams:\n",
      ".     coinbase account\n",
      ".     western union\n",
      ".   Most correlated trigrams:\n",
      ".     money western union\n",
      ".     called western union\n",
      "# 'mortgage':\n",
      ".   Most correlated unigrams:\n",
      ".     modification\n",
      ".     mortgage\n",
      ".   Most correlated bigrams:\n",
      ".     mortgage company\n",
      ".     loan modification\n",
      ".   Most correlated trigrams:\n",
      ".     loan servicing llc\n",
      ".     ocwen loan servicing\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import data\n",
    "dta = pd.read_csv(\"D:/Documents/Data/case_study_data_copy.csv\")\n",
    "\n",
    "# Choose columns to be used\n",
    "corps = dta[['product_group','text']]\n",
    "\n",
    "# Set X and y columns\n",
    "X = corps.text\n",
    "y = corps.product_group\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "# Transform charcater labels ti numbers and back again\n",
    "df = corps\n",
    "df = df[pd.notnull(df['product_group'])]\n",
    "df.columns = ['product_group','text']\n",
    "df['category_id'] = df['product_group'].factorize()[0]\n",
    "category_id_df = df[['product_group', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'product_group']].values)\n",
    "df.head()\n",
    "\n",
    "# Use TfidfVectorizer to tokenize the text, remove stopwords, convert to lowercase, build vocabulary, etc.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVectorizer(\n",
    "    input='corps',            #the input text data or corpus\n",
    "    encoding='utf-8',         #encoding is used to decode\n",
    "    decode_error='strict',    #means a UnicodeDecodeError will be raised (other values are ignore and replace) \n",
    "    strip_accents='ascii',    #removes accents and perform other character normalization (ascii is the fastest)\n",
    "    lowercase=True,           #converts all tect to lower case\n",
    "    tokenizer=word_tokenize,  #default value is None (only applies if analyzer == 'word')\n",
    "    stop_words='english',     #default value is None (only applies if analyzer == 'word')\n",
    "    token_pattern=r'\\b\\w+\\b', #string, denoting what constitutes a “token” (only used if analyzer == 'word')\n",
    "    ngram_range=(1, 3),       #will yield unigrams, bigrams, and trigrams\n",
    "    analyzer='word',          #feature makeup {‘string’, ‘word’, ‘char’, ‘char_wb’} or callable\n",
    "    max_df=1.0,               #ignore terms that have a frequency higher than this threshold\n",
    "    min_df=1,                 #ignore terms that have a frequency lower than this threshold\n",
    "    max_features = 20,        #build a vocabulary size N or None\n",
    "    binary=False,             #if True, all non zero counts are set to 1\n",
    "    dtype= np.int64,          #type of the matrix returned by fit_transform() or transform()\n",
    "    norm='l2',                #each output row will have unit norm, either: 'l2', 'l1', or None\n",
    "    use_idf=True,             #enable inverse-document-frequency reweighting\n",
    "    smooth_idf=True,          #smooth idf weights by adding one to document frequencies to prevent divison by zero\n",
    "    sublinear_tf=False,       #apply sublinear tf scaling, i.e. replace tf with 1 + log(tf)\n",
    ")\n",
    "\n",
    "tfidf= TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 3), stop_words='english')\n",
    "features = tfidf.fit_transform(X_train) #CORRECT TRANSFORM\n",
    "labels = y_train #CORRECT LABEL\n",
    "\n",
    "# Extract N-grams: unigrams, bigrams, and trigrams\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "for product_group, category_id in sorted(id_to_category.items()):\n",
    "  features_chi2 = chi2(features,labels == category_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
    "  print(\"# '{}':\".format(category_id))\n",
    "  print(\".   Most correlated unigrams:\\n.     {}\".format('\\n.     '.join(unigrams[-N:])))\n",
    "  print(\".   Most correlated bigrams:\\n.     {}\".format('\\n.     '.join(bigrams[-N:])))\n",
    "  print(\".   Most correlated trigrams:\\n.     {}\".format('\\n.     '.join(trigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
